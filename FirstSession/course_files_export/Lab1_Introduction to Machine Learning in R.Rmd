---
title: "Lab1 – Introduction to Machine Learning in R"
author: "Dr. Reza Arabi Belaghi"
output:
  word_document:
    toc: true
    toc_depth: '2'
  html_document:
    toc: true
    toc_depth: 2
    number_sections: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
set.seed(123)  # for reproducibility
```

# 1. Overview

In this lab, you will:

- Review the basic workflow of **supervised learning** using **logistic regression**.
- Compare **classical logistic regression** with **penalized methods**: LASSO, Ridge, and Elastic Net.
- Explore a **high-dimensional setting** where the number of predictors is larger than the number of observations (**p > n**).
- Use **PCA (Principal Component Analysis)** as an unsupervised tool for dimensionality reduction.
- Fit penalized models on high-dimensional data.
- Work through one **real-data example** using a publicly available dataset.

We will use the R packages **glmnet** (for penalized regression) and **ISLR** (for a real dataset).  
If needed, install them before running the lab:

```r
# install.packages("glmnet")
# install.packages("ISLR")
```

# 2. Example 1 – Logistic Regression & Penalized Methods (Supervised ML)

In this example, we simulate a binary outcome and a few predictors. We then:

1. Fit a **standard logistic regression** model.
2. Fit **LASSO**, **Ridge**, and **Elastic Net** models.
3. Compare their performance.

## 2.1 Simulate a binary classification dataset

We simulate 3 predictors where \(X_1\) and \(X_2\) are truly associated with the outcome, and \(X_3\) is mostly noise.

```{r simulate_example1}
n <- 500

x1 <- rnorm(n, mean = 0, sd = 1)
x2 <- rnorm(n, mean = 1, sd = 1.5)
x3 <- rnorm(n, mean = 0, sd = 1)  # mostly noise

# True linear predictor
eta <- -0.5 + 1.2 * x1 - 1.0 * x2 + 0.1 * x3
prob <- 1 / (1 + exp(-eta))

y <- rbinom(n, size = 1, prob = prob)

data1 <- data.frame(y = factor(y), x1, x2, x3)
head(data1)
```

## 2.2 Train–test split

We split the data into training (70%) and test (30%) sets.

```{r split_example1}
set.seed(123)
train_index <- sample(seq_len(nrow(data1)), size = 0.7 * nrow(data1))

train1 <- data1[train_index, ]
test1  <- data1[-train_index, ]

nrow(train1); nrow(test1)
```

## 2.3 Standard logistic regression (GLM)

```{r glm_example1}
fit_glm <- glm(y ~ x1 + x2 + x3,
               data = train1,
               family = binomial)

summary(fit_glm)
```

Make predictions and evaluate accuracy on the test set:

```{r glm_pred_example1}
# Predict probabilities
pred_prob_glm <- predict(fit_glm, newdata = test1, type = "response")

# Classify using 0.5 cutoff
pred_class_glm <- ifelse(pred_prob_glm > 0.5, 1, 0)

# Confusion matrix
table(Predicted = pred_class_glm, Observed = test1$y)

# Accuracy
accuracy_glm <- mean(pred_class_glm == as.numeric(as.character(test1$y)))
accuracy_glm
```

 **Question 1 (for students):**  
 - Which predictors appear statistically significant in the logistic regression output?  
 - How does this relate to how we simulated the data?

## 2.4 Penalized logistic regression: LASSO, Ridge, Elastic Net

We now fit penalized models using the **glmnet** package.

```{r glmnet_setup, message=FALSE}
library(glmnet)

# glmnet expects matrix of predictors and numeric outcome (0/1)
x_train <- as.matrix(train1[, c("x1", "x2", "x3")])
y_train <- as.numeric(as.character(train1$y))

x_test  <- as.matrix(test1[, c("x1", "x2", "x3")])
y_test  <- as.numeric(as.character(test1$y))
```

### 2.4.1 LASSO (L1 penalty)

```{r lasso_example1}
# alpha = 1 => LASSO
cv_lasso <- cv.glmnet(x_train, y_train,
                      family = "binomial",
                      alpha = 1,
                      nfolds = 10)

plot(cv_lasso)
cv_lasso$lambda.min  # best lambda
coef(cv_lasso, s = "lambda.min")
```

Evaluate LASSO on the test set:

```{r lasso_pred_example1}
pred_prob_lasso <- predict(cv_lasso, newx = x_test, s = "lambda.min", type = "response")
pred_class_lasso <- ifelse(pred_prob_lasso > 0.5, 1, 0)

table(Predicted = pred_class_lasso, Observed = y_test)
accuracy_lasso <- mean(pred_class_lasso == y_test)
accuracy_lasso
```

### 2.4.2 Ridge (L2 penalty)

```{r ridge_example1}
# alpha = 0 => Ridge
cv_ridge <- cv.glmnet(x_train, y_train,
                      family = "binomial",
                      alpha = 0,
                      nfolds = 10)

plot(cv_ridge)
cv_ridge$lambda.min
coef(cv_ridge, s = "lambda.min")
```

Evaluate Ridge:

```{r ridge_pred_example1}
pred_prob_ridge <- predict(cv_ridge, newx = x_test, s = "lambda.min", type = "response")
pred_class_ridge <- ifelse(pred_prob_ridge > 0.5, 1, 0)

table(Predicted = pred_class_ridge, Observed = y_test)
accuracy_ridge <- mean(pred_class_ridge == y_test)
accuracy_ridge
```

### 2.4.3 Elastic Net (mix of L1 and L2)

We choose \(\alpha = 0.5\) for a 50/50 mixture, but this can be tuned.

```{r enet_example1}
alpha_enet <- 0.5

cv_enet <- cv.glmnet(x_train, y_train,
                     family = "binomial",
                     alpha = alpha_enet,
                     nfolds = 10)

plot(cv_enet)
cv_enet$lambda.min
coef(cv_enet, s = "lambda.min")
```

Evaluate Elastic Net:

```{r enet_pred_example1}
pred_prob_enet <- predict(cv_enet, newx = x_test, s = "lambda.min", type = "response")
pred_class_enet <- ifelse(pred_prob_enet > 0.5, 1, 0)

table(Predicted = pred_class_enet, Observed = y_test)
accuracy_enet <- mean(pred_class_enet == y_test)
accuracy_enet
```

### 2.5 Compare model performances

```{r compare_accuracy_example1}
data.frame(
  Model = c("Logistic (GLM)", "LASSO", "Ridge", "Elastic Net"),
  Accuracy = c(accuracy_glm, accuracy_lasso, accuracy_ridge, accuracy_enet)
)
```

 **Question 2 (for students):**
 - Which model has the highest accuracy?  
 - Look at the LASSO coefficients: did it shrink any variable close to zero?  
 - How does penalization help when we have many predictors or multicollinearity?

# 3. Example 2 – High-Dimensional Data (p > n), PCA & Penalization

In this example, we:

- Simulate a **high-dimensional dataset** with \(n = 80\) observations and \(p = 200\) predictors.
- Use **PCA** (unsupervised) for dimensionality reduction.
- Fit a **logistic regression** model using the leading principal components.
- Fit a **LASSO logistic regression** directly on the high-dimensional predictors.

This illustrates why ML (and penalized methods) are useful when **p > n**.

## 3.1 Simulate p > n data

We let only a few predictors drive the true signal, and the rest act as noise.

```{r simulate_example2}
n_hd <- 80
p_hd <- 200

# Design matrix: standard normal
X_hd <- matrix(rnorm(n_hd * p_hd), nrow = n_hd, ncol = p_hd)

# True coefficients: only first 5 are nonzero
true_beta <- c(1.5, -1.0, 0.8, -0.7, 1.2, rep(0, p_hd - 5))

eta_hd <- X_hd %*% true_beta
prob_hd <- 1 / (1 + exp(-eta_hd))
y_hd <- rbinom(n_hd, size = 1, prob = prob_hd)

y_hd <- as.numeric(y_hd)
dim(X_hd)
head(y_hd)
```

Train–test split:

```{r split_example2}
set.seed(123)
train_index_hd <- sample(seq_len(n_hd), size = 0.7 * n_hd)

X_train_hd <- X_hd[train_index_hd, ]
X_test_hd  <- X_hd[-train_index_hd, ]

y_train_hd <- y_hd[train_index_hd]
y_test_hd  <- y_hd[-train_index_hd]

length(y_train_hd); length(y_test_hd)
```

## 3.2 PCA (unsupervised step)

PCA is an **unsupervised** method: it ignores the outcome and tries to summarize the variance in the predictors.

```{r pca_example2}
pca_hd <- prcomp(X_train_hd, center = TRUE, scale. = TRUE)

# Proportion of variance explained
pve <- pca_hd$sdev^2 / sum(pca_hd$sdev^2)

plot(pve[1:20], type = "b",
     xlab = "Principal Component",
     ylab = "Proportion of Variance Explained",
     main = "Scree Plot (first 20 PCs)")
```

Select enough PCs to explain, say, ~80–90% of the variance:

```{r choose_pcs_example2}
cum_pve <- cumsum(pve)
which(cum_pve >= 0.80)[1]  # first PC index reaching 80%
which(cum_pve >= 0.90)[1]  # first PC index reaching 90%
```

Let’s take the first 20 PCs (for illustration).

```{r pca_scores_example2}
k <- 20
Z_train <- pca_hd$x[, 1:k]

# Get PC scores for the test data
Z_test <- predict(pca_hd, newdata = X_test_hd)[, 1:k]

dim(Z_train); dim(Z_test)
```

## 3.3 Logistic regression on PCs (supervised model after PCA)

```{r glm_pca_example2}
pca_data_train <- data.frame(y = factor(y_train_hd), Z_train)
pca_data_test  <- data.frame(y = factor(y_test_hd), Z_test)

fit_glm_pca <- glm(y ~ ., data = pca_data_train, family = binomial)
summary(fit_glm_pca)
```

Evaluate performance:

```{r glm_pca_pred_example2}
pred_prob_pca_glm <- predict(fit_glm_pca, newdata = pca_data_test, type = "response")
pred_class_pca_glm <- ifelse(pred_prob_pca_glm > 0.5, 1, 0)

table(Predicted = pred_class_pca_glm, Observed = y_test_hd)
accuracy_pca_glm <- mean(pred_class_pca_glm == y_test_hd)
accuracy_pca_glm
```

**Question 3 (for students):**
- Why was it helpful (or necessary) to reduce dimensionality with PCA before fitting a logistic regression in this p > n setting?  
 - Is PCA supervised or unsupervised? Explain.

## 3.4 LASSO directly on high-dimensional predictors

Now we skip PCA and fit a LASSO model directly on \(X\). This is a typical ML approach in high-dimensional data.

```{r lasso_hd_example2}
cv_lasso_hd <- cv.glmnet(X_train_hd, y_train_hd,
                         family = "binomial",
                         alpha = 1,
                         nfolds = 10)

plot(cv_lasso_hd)
cv_lasso_hd$lambda.min
```

Look at selected coefficients:

```{r lasso_hd_coefs_example2}
coef_lasso_hd <- coef(cv_lasso_hd, s = "lambda.min")
coef_lasso_hd[1:15, ]  # first 15 coefficients (including intercept)
```

Evaluate performance:

```{r lasso_hd_pred_example2}
pred_prob_lasso_hd <- predict(cv_lasso_hd, newx = X_test_hd, s = "lambda.min", type = "response")
pred_class_lasso_hd <- ifelse(pred_prob_lasso_hd > 0.5, 1, 0)

table(Predicted = pred_class_lasso_hd, Observed = y_test_hd)
accuracy_lasso_hd <- mean(pred_class_lasso_hd == y_test_hd)
accuracy_lasso_hd
```

## 3.5 Compare PCA+GLM vs LASSO in high-dimensional setting

```{r compare_hd_example2}
data.frame(
  Model = c("GLM on first 20 PCs", "LASSO on all 200 predictors"),
  Accuracy = c(accuracy_pca_glm, accuracy_lasso_hd)
)
```

> **Question 4 (for students):**
> - Which approach performs better in this simulation: PCA + logistic regression or LASSO on all predictors?  
> - How might your choice depend on interpretability vs predictive performance in a real biomedical dataset?

# 4. Example 3 – Real Data: Credit Default (ISLR::Default)

In this final example, we use a **real dataset** from the **ISLR** package: `Default`, which contains information on individuals and whether they defaulted on their credit card debt.

We will:

1. Explore the data.
2. Fit a standard logistic regression model.
3. Fit a LASSO logistic regression model.
4. Compare predictions and discuss when ML adds value.

## 4.1 Load and explore the data

```{r load_real_data}
library(ISLR)  # may need: install.packages("ISLR")

data("Default")
str(Default)
head(Default)
summary(Default)
```

The main variables are:

- `default`: Yes/No (outcome)
- `student`: Yes/No
- `balance`: average credit card balance
- `income`: income

We create a binary outcome (`y = 1` if default == "Yes").

```{r prepare_real_data}
Default$y <- ifelse(Default$default == "Yes", 1, 0)

# Simple exploratory plots
plot(Default$balance, Default$y,
     xlab = "Balance", ylab = "Default (0/1)",
     main = "Credit Card Balance vs Default")

table(Default$default, Default$student)
```

## 4.2 Train–test split

```{r split_real_data}
set.seed(123)
n_def <- nrow(Default)
train_index_def <- sample(seq_len(n_def), size = 0.7 * n_def)

train_def <- Default[train_index_def, ]
test_def  <- Default[-train_index_def, ]

nrow(train_def); nrow(test_def)
```

## 4.3 Standard logistic regression

```{r glm_real}
fit_glm_def <- glm(y ~ student + balance + income,
                   data = train_def,
                   family = binomial)

summary(fit_glm_def)
```

Predict and evaluate:

```{r glm_real_pred}
pred_prob_glm_def <- predict(fit_glm_def, newdata = test_def, type = "response")
pred_class_glm_def <- ifelse(pred_prob_glm_def > 0.5, 1, 0)

table(Predicted = pred_class_glm_def, Observed = test_def$y)
accuracy_glm_def <- mean(pred_class_glm_def == test_def$y)
accuracy_glm_def
```

Because default is rare, accuracy alone can be misleading. Compute sensitivity and specificity:

```{r glm_real_metrics}
glm_cm <- table(Predicted = pred_class_glm_def, Observed = test_def$y)

tn <- glm_cm["0","0"]
tp <- glm_cm["1","1"]
fn <- glm_cm["0","1"]
fp <- glm_cm["1","0"]

sensitivity_glm <- tp / (tp + fn)
specificity_glm <- tn / (tn + fp)

c(Accuracy = accuracy_glm_def,
  Sensitivity = sensitivity_glm,
  Specificity = specificity_glm)
```

## 4.4 LASSO logistic regression on real data

We now treat this as a simple ML problem with penalization. For illustration, we include interaction terms and let LASSO decide which to keep.

```{r lasso_real_setup}
# Create model matrix with main effects and interaction
x_def <- model.matrix(y ~ student * (balance + income), data = Default)[, -1]  # remove intercept
y_def <- Default$y

# Train–test split aligned with previous split
x_train_def <- x_def[train_index_def, ]
x_test_def  <- x_def[-train_index_def, ]
y_train_def <- y_def[train_index_def]
y_test_def  <- y_def[-train_index_def]
```

Fit LASSO:

```{r lasso_real_fit}
cv_lasso_def <- cv.glmnet(x_train_def, y_train_def,
                          family = "binomial",
                          alpha = 1,
                          nfolds = 10)

plot(cv_lasso_def)
cv_lasso_def$lambda.min
coef(cv_lasso_def, s = "lambda.min")
```

Predict and evaluate:

```{r lasso_real_pred}
pred_prob_lasso_def <- predict(cv_lasso_def, newx = x_test_def, s = "lambda.min", type = "response")
pred_class_lasso_def <- ifelse(pred_prob_lasso_def > 0.5, 1, 0)

lasso_cm <- table(Predicted = pred_class_lasso_def, Observed = y_test_def)
lasso_cm

accuracy_lasso_def <- mean(pred_class_lasso_def == y_test_def)

tn_l <- lasso_cm["0","0"]
tp_l <- lasso_cm["1","1"]
fn_l <- lasso_cm["0","1"]
fp_l <- lasso_cm["1","0"]

sensitivity_lasso <- tp_l / (tp_l + fn_l)
specificity_lasso <- tn_l / (tn_l + fp_l)

c(Accuracy = accuracy_lasso_def,
  Sensitivity = sensitivity_lasso,
  Specificity = specificity_lasso)
```

## 4.5 Compare logistic regression vs LASSO on real data

```{r compare_real_models}
data.frame(
  Model = c("GLM", "LASSO"),
  Accuracy = c(accuracy_glm_def, accuracy_lasso_def),
  Sensitivity = c(sensitivity_glm, sensitivity_lasso),
  Specificity = c(specificity_glm, specificity_lasso)
)
```

 **Questions for students (Real Data):**
 1. Which model (GLM vs LASSO) performs better in terms of sensitivity and specificity?
 2. Look at the LASSO coefficients: which variables or interactions are shrunk toward zero?
3. How would you explain the added value of penalized ML methods to a clinician or non-statistical collaborator?

# 5. Reflection

1. In Example 1, how did penalized methods compare to the standard logistic regression in terms of:
   - Accuracy?
   - Coefficient shrinkage and variable selection?

2. In Example 2, why are ML methods like LASSO (and dimensionality reduction via PCA) especially useful when we have **more variables than observations**?

3. In the real-data example (Example 3):
   - Does penalization improve sensitivity or specificity compared to the standard GLM?
   - How might class imbalance affect your model evaluation?

4. Think about your own research data:
   - Where might **supervised learning** (e.g., penalized logistic regression) be appropriate?
   - Where might **unsupervised learning** (e.g., PCA or clustering) be more useful?

---

**End of Lab**

You can modify the simulation settings (sample size, number of predictors, signal strength) to explore how model performance changes, and you can replace the real dataset with one from your own research domain.
